#!/usr/bin/env python3
"""
å•†å“æ ‡ç­¾æå–æ¼”ç¤º
å±•ç¤ºjiebaåˆ†è¯å’Œæ ‡ç­¾æå–çš„è¯¦ç»†è¿‡ç¨‹
"""

import sys
import os
sys.path.append('backend')

import jieba
import jieba.posseg as pseg

def demo_tag_extraction():
    """æ¼”ç¤ºæ ‡ç­¾æå–è¿‡ç¨‹"""
    
    print("=" * 80)
    print("å•†å“æ ‡ç­¾æå–è¯¦ç»†æ¼”ç¤º")
    print("=" * 80)
    
    # ç¤ºä¾‹å•†å“æ ‡é¢˜
    sample_titles = [
        "ç»å…¸æ•…äº‹ å¥³è£… ç§‹è£… çƒ­å– æ—¶å°š ç®€çº¦ çº¯è‰² æ£‰è´¨ è¤¶çš±çŸ­è£™ åŠèº«è£™ A å­— ç¾¤ Q 790",
        "iPhone 15 Pro Max 256GB æ·±ç©ºé»‘è‰²",
        "Nike Air Max 270 è¿åŠ¨é‹ ç”·æ¬¾ ç™½è‰²",
        "MacBook Pro 14è‹±å¯¸ M3èŠ¯ç‰‡ æ·±ç©ºç°è‰²",
        "Sony WH-1000XM5 é™å™ªè€³æœº æ— çº¿è“ç‰™"
    ]
    
    # åœç”¨è¯è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
    stop_words = {
        'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä¸ª', 'ä»¬', 'ä¸­', 'æ¥', 'ç”¨', 'å¹´', 'æœˆ', 'æ—¥', 'æ—¶', 'åˆ†', 'ç§’',
        'å…ƒ', 'å—', 'é’±', 'åŒ…', 'ä»¶', 'ä¸ª', 'åª', 'åŒ', 'æ¡', 'å¥—', 'å°', 'éƒ¨', 'å¼ ', 'æœ¬', 'æ”¯', 'ç“¶', 'ç›’', 'è¢‹', 'ç®±', 'åŒ…', 'æ–¤', 'å…‹', 'å‡', 'ç±³', 'å˜ç±³', 'æ¯«ç±³', 'å¯¸', 'å°º', 'ç ', 'å·', 'è‰²', 'æ¬¾', 'å‹', 'ç‰ˆ', 'å¼', 'æ ·', 'ç±»', 'ç§', 'å“', 'ç‰Œ', 'å', 'ç§°', 'ä»·', 'æ ¼',
        'ç‰¹', 'ä»·', 'ä¼˜', 'æƒ ', 'æŠ˜', 'æ‰£', 'å…', 'è´¹', 'åŒ…', 'é‚®', 'æ­£', 'å“', 'åŸ', 'è£…', 'è¿›', 'å£', 'å›½', 'äº§', 'æ–°', 'æ¬¾', 'è€', 'æ¬¾', 'çƒ­', 'å–', 'çˆ†', 'æ¬¾', 'é™', 'é‡', 'ç‰¹', 'ä¾›', 'ä¸“', 'ä¾›', 'ç‹¬', 'å®¶', 'é¦–', 'å‘', 'æŠ¢', 'è´­', 'ç§’', 'æ€', 'å›¢', 'è´­', 'æ‹¼', 'å›¢', 'ç ', 'ä»·'
    }
    
    # æœ‰æ„ä¹‰çš„è¯æ€§
    meaningful_pos = {
        'n',      # åè¯
        'nr',     # äººå
        'ns',     # åœ°å
        'nt',     # æœºæ„å›¢ä½“
        'nw',     # ä½œå“å
        'nz',     # å…¶ä»–ä¸“å
        'v',      # åŠ¨è¯
        'vn',     # ååŠ¨è¯
        'a',      # å½¢å®¹è¯
        'ad',     # å‰¯å½¢è¯
        'an',     # åå½¢è¯
        'eng'     # è‹±æ–‡
    }
    
    for i, title in enumerate(sample_titles, 1):
        print(f"\nğŸ“ ç¤ºä¾‹ {i}: {title}")
        print("-" * 60)
        
        # 1. ç®€å•åˆ†è¯
        print("1ï¸âƒ£ ç®€å•åˆ†è¯ç»“æœ:")
        simple_words = list(jieba.cut(title))
        print(f"   {simple_words}")
        
        # 2. è¯æ€§æ ‡æ³¨åˆ†è¯
        print("\n2ï¸âƒ£ è¯æ€§æ ‡æ³¨åˆ†è¯:")
        pos_words = list(pseg.cut(title))
        for word, flag in pos_words:
            print(f"   '{word}' -> {flag}")
        
        # 3. æ ‡ç­¾æå–è¿‡ç¨‹
        print("\n3ï¸âƒ£ æ ‡ç­¾æå–è¿‡ç¨‹:")
        tags = []
        filtered_words = []
        
        for word, flag in pos_words:
            # æ£€æŸ¥æ¡ä»¶
            length_ok = len(word) > 1
            not_stopword = word not in stop_words
            meaningful_pos_flag = flag in meaningful_pos
            not_digit = not word.isdigit()
            
            print(f"   '{word}' ({flag}): é•¿åº¦>1:{length_ok}, éåœç”¨è¯:{not_stopword}, æœ‰æ„ä¹‰è¯æ€§:{meaningful_pos_flag}, éçº¯æ•°å­—:{not_digit}")
            
            if length_ok and not_stopword and meaningful_pos_flag and not_digit:
                tags.append(word)
                filtered_words.append(f"{word}({flag})")
            else:
                print(f"      âŒ è¢«è¿‡æ»¤")
        
        # 4. æœ€ç»ˆç»“æœ
        print(f"\n4ï¸âƒ£ æœ€ç»ˆæå–çš„æ ‡ç­¾:")
        print(f"   è¿‡æ»¤åçš„è¯æ±‡: {filtered_words}")
        print(f"   æœ€ç»ˆæ ‡ç­¾: {tags}")
        print(f"   æ ‡ç­¾æ•°é‡: {len(tags)}")
        
        print("\n" + "="*60)

def demo_word_frequency():
    """æ¼”ç¤ºè¯é¢‘ç»Ÿè®¡"""
    print("\n" + "=" * 80)
    print("æ ‡ç­¾è¯é¢‘ç»Ÿè®¡æ¼”ç¤º")
    print("=" * 80)
    
    # æ¨¡æ‹Ÿä¸€äº›å•†å“æ ‡é¢˜
    titles = [
        "ç»å…¸æ•…äº‹ å¥³è£… ç§‹è£… çƒ­å– æ—¶å°š ç®€çº¦ çº¯è‰² æ£‰è´¨ è¤¶çš±çŸ­è£™",
        "éŸ©ç‰ˆ å¥³è£… æ—¶å°š è¿è¡£è£™ å¤å­£ æ–°æ¬¾ åŒ…é‚®",
        "ç”·è£… ä¼‘é—² è¡¬è¡« å•†åŠ¡ æ­£è£… ç™½è‰² æ£‰è´¨",
        "è¿åŠ¨é‹ è·‘æ­¥ ç”·æ¬¾ ç™½è‰² é€æ°” èˆ’é€‚",
        "æ‰‹æœº è‹¹æœ iPhone 15 Pro Max æ·±ç©ºé»‘è‰²"
    ]
    
    # ç»Ÿè®¡æ‰€æœ‰æ ‡ç­¾
    all_tags = []
    for title in titles:
        words = list(pseg.cut(title))
        for word, flag in words:
            if (len(word) > 1 and 
                word not in {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä¸ª', 'ä»¬', 'ä¸­', 'æ¥', 'ç”¨', 'å¹´', 'æœˆ', 'æ—¥', 'æ—¶', 'åˆ†', 'ç§’', 'å…ƒ', 'å—', 'é’±', 'åŒ…', 'ä»¶', 'åª', 'åŒ', 'æ¡', 'å¥—', 'å°', 'éƒ¨', 'å¼ ', 'æœ¬', 'æ”¯', 'ç“¶', 'ç›’', 'è¢‹', 'ç®±', 'åŒ…', 'æ–¤', 'å…‹', 'å‡', 'ç±³', 'å˜ç±³', 'æ¯«ç±³', 'å¯¸', 'å°º', 'ç ', 'å·', 'è‰²', 'æ¬¾', 'å‹', 'ç‰ˆ', 'å¼', 'æ ·', 'ç±»', 'ç§', 'å“', 'ç‰Œ', 'å', 'ç§°', 'ä»·', 'æ ¼', 'ç‰¹', 'ä»·', 'ä¼˜', 'æƒ ', 'æŠ˜', 'æ‰£', 'å…', 'è´¹', 'åŒ…', 'é‚®', 'æ­£', 'å“', 'åŸ', 'è£…', 'è¿›', 'å£', 'å›½', 'äº§', 'æ–°', 'æ¬¾', 'è€', 'æ¬¾', 'çƒ­', 'å–', 'çˆ†', 'æ¬¾', 'é™', 'é‡', 'ç‰¹', 'ä¾›', 'ä¸“', 'ä¾›', 'ç‹¬', 'å®¶', 'é¦–', 'å‘', 'æŠ¢', 'è´­', 'ç§’', 'æ€', 'å›¢', 'è´­', 'æ‹¼', 'å›¢', 'ç ', 'ä»·'} and
                flag in ['n', 'nr', 'ns', 'nt', 'nw', 'nz', 'v', 'vn', 'a', 'ad', 'an', 'eng'] and
                not word.isdigit()):
                all_tags.append(word)
    
    # ç»Ÿè®¡è¯é¢‘
    from collections import Counter
    tag_counts = Counter(all_tags)
    
    print("æ ‡ç­¾è¯é¢‘ç»Ÿè®¡:")
    for tag, count in tag_counts.most_common(10):
        print(f"   {tag}: {count}æ¬¡")

if __name__ == "__main__":
    demo_tag_extraction()
    demo_word_frequency()
